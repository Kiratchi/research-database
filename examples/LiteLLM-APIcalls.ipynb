{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c95851",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install litellm python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4204402c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import litellm\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import sys\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b30d895f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Get values from environment\n",
    "LITELLM_API_KEY = os.getenv(\"LITELLM_API_KEY\")\n",
    "LITELLM_BASE_URL = os.getenv(\"LITELLM_BASE_URL\")\n",
    "\n",
    "# Validate environment variables\n",
    "if not LITELLM_API_KEY or not LITELLM_BASE_URL:\n",
    "    print(\"Error: Missing API key or base URL in .env file\")\n",
    "    sys.exit(1)\n",
    "\n",
    "litellm.api_base = LITELLM_BASE_URL\n",
    "litellm.api_key = LITELLM_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56111268",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_available_models():\n",
    "    \"\"\"Get list of available models from your LiteLLM proxy\"\"\"\n",
    "    \n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {LITELLM_API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(\n",
    "            f\"{LITELLM_BASE_URL}/models\",\n",
    "            headers=headers\n",
    "        )\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            models = response.json()\n",
    "            return [model[\"id\"] for model in models[\"data\"]]\n",
    "        else:\n",
    "            return f\"Error: {response.status_code} - {response.text}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d07dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_model(model_name, message, stream=False):\n",
    "    \"\"\"Chat with any available model through LiteLLM proxy\"\"\"\n",
    "    try:\n",
    "        response = litellm.completion(\n",
    "            model=model_name,\n",
    "            messages=[{\"role\": \"user\", \"content\": message}],\n",
    "            api_key=LITELLM_API_KEY,\n",
    "            api_base=LITELLM_BASE_URL,\n",
    "            stream=stream\n",
    "        )\n",
    "        \n",
    "        if stream:\n",
    "            full_response = \"\"\n",
    "            for chunk in response:\n",
    "                if chunk.choices[0].delta.content:\n",
    "                    content = chunk.choices[0].delta.content\n",
    "                    print(content, end=\"\", flush=True)\n",
    "                    full_response += content\n",
    "            print()\n",
    "            return full_response\n",
    "        else:\n",
    "            return response.choices[0].message.content\n",
    "            \n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edcc52f",
   "metadata": {},
   "source": [
    "### Check available models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aba4a982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models:\n",
      "- o1\n",
      "- gpt-4.5-preview\n",
      "- claude-haiku-3.5\n",
      "- gpt-4.1-2025-04-14\n",
      "- claude-sonet-3.7\n",
      "- o3-mini-2025-01-31\n",
      "- claude-opus-4\n",
      "- claude-sonet-4\n",
      "- gpt-4o-search-preview-2025-03-11\n",
      "- openai/hd/1024-x-1024/dall-e-3\n"
     ]
    }
   ],
   "source": [
    "print(\"Available models:\")\n",
    "models = get_available_models()\n",
    "if isinstance(models, list):\n",
    "    for model in models[:10]:  # Show first 10 models\n",
    "        print(f\"- {model}\")\n",
    "else:\n",
    "    print(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fe3bfc",
   "metadata": {},
   "source": [
    "### Test basic chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae8c32d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with model: anthropic/claude-haiku-3.5\n",
      "Query: What goes well with pancakes besides jam?\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Response: Here are some delicious toppings and accompaniments for pancakes:\n",
      "\n",
      "1. Sweet options:\n",
      "- Maple syrup\n",
      "- Honey\n",
      "- Whipped cream\n",
      "- Fresh berries\n",
      "- Nutella\n",
      "- Chocolate chips\n",
      "- Caramel sauce\n",
      "- Butter\n",
      "- Powdered sugar\n",
      "\n",
      "2. Fruit options:\n",
      "- Sliced bananas\n",
      "- Strawberries\n",
      "- Blueberries\n",
      "- Apple compote\n",
      "- Peaches\n",
      "- Raspberries\n",
      "\n",
      "3. Savory options:\n",
      "- Bacon\n",
      "- Scrambled eggs\n",
      "- Sausage\n",
      "- Ham\n",
      "- Cheese\n",
      "- Fried chicken\n",
      "- Smoked salmon\n",
      "\n",
      "4. Spreads:\n",
      "- Peanut butter\n",
      "- Almond butter\n",
      "- Cream cheese\n",
      "- Yogurt\n",
      "\n",
      "5. Nuts and seeds:\n",
      "- Chopped almonds\n",
      "- Pecans\n",
      "- Walnuts\n",
      "- Chia seeds\n",
      "- Sunflower seeds\n",
      "\n",
      "Each of these can add different flavors and textures to your pancakes, making them more interesting and enjoyable.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"anthropic/claude-haiku-3.5\" # Check https://docs.litellm.ai/docs/providers for how to state model name depending on provider\n",
    "message = \"What goes well with pancakes besides jam?\"\n",
    "    \n",
    "print(f\"Testing with model: {model_name}\")\n",
    "print(f\"Query: {message}\")\n",
    "    \n",
    "result = chat_with_model(model_name, message)\n",
    "print(f\"Response: {result}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (03_CoAuthor)",
   "language": "python",
   "name": "03_coauthor_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
